# ğŸ§  Self-Correcting Multi-Agent AI Research System

A full-stack, local-first AI application where multiple intelligent agents collaborate to research, critique, and refine content. The system runs entirely on **local LLMs** (no API keys, no cloud costs) and includes a **FastAPI backend** with a simple, user-friendly **web frontend**.

---

## ğŸŒŸ Highlights

* ğŸ¤– Multi-agent AI workflow (Researcher, Critic, Editor)
* ğŸ” Controlled self-correction loops (no infinite recursion)
* ğŸ’» Runs fully offline using Ollama (Gemma 2B)
* âš¡ FastAPI backend with REST endpoint
* ğŸŒ Clean, simple web frontend
* ğŸ§  Optimized for low-memory machines
* ğŸ§© Modular and extensible design

---

## â„¹ï¸ Overview

This project demonstrates how **multiple AI agents** can collaborate in a structured pipeline to produce higher-quality research output.

Instead of relying on a single prompt, the system uses specialized agents with distinct roles:

* **Researcher** creates an initial draft
* **Critic** reviews the draft for major factual or logical issues
* **Editor** refines the content into a polished final report

The system runs for a **controlled number of refinement loops** to improve quality while guaranteeing termination.

All language models are hosted **locally** using **Ollama**, making the project:

* Cost-free
* Privacy-friendly
* Fully offline
* Easy to run on personal machines

---



## ğŸ› ï¸ Tech Stack

| Component           | Tool                       |
| ------------------- | -------------------------- |
| LLM                 | Ollama (Gemma 2B)          |
| Agent Orchestration | LangGraph                  |
| Backend             | FastAPI + Uvicorn          |
| Frontend            | HTML + JavaScript          |
| Language            | Python 3.11                |
| Environment         | Virtual Environment (venv) |

---

## ğŸ“ Project Structure

```
self_correcting_agents/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ researcher.py
â”‚   â”œâ”€â”€ critic.py
â”‚   â””â”€â”€ editor.py
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ graph.py
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ venv/
```

---

## âš™ï¸ How It Works

```
User â†’ Web UI â†’ FastAPI â†’ LangGraph Agents â†’ Final Report â†’ UI
```

Agent pipeline:

```
Researcher â†’ Critic â†’ (Repeat a few times) â†’ Editor â†’ Final Output
```

This ensures better quality output while preventing infinite loops.

---

## ğŸš€ Usage

1. Enter a topic in the web interface
2. Click **Generate Report**
3. The system runs the multi-agent pipeline
4. A refined research report is returned

The interface is intentionally simple so users can focus on the results.

---

## â¬‡ï¸ Installation

### 1. Install Ollama

[https://ollama.com](https://ollama.com)

### 2. Pull the model

```bash
ollama pull gemma:2b
```

### 3. Start Ollama

```bash
ollama run gemma:2b
```

### 4. Activate the virtual environment

```bash
venv\Scripts\activate
```

### 5. Install dependencies

```bash
pip install langgraph langchain langchain-ollama fastapi uvicorn python-multipart ddgs
```

### 6. Run the backend

```bash
uvicorn backend.app:app --reload
```

### 7. Open the frontend

Open:

```
frontend/index.html
```

---



## âœï¸ Author

**Manasvi**
AI & Software Engineering Enthusiast

---

